{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "#为了minibatch的功能\n",
    "import torch.utils.data as Data\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.common import Conv, Bottleneck, SPP, DWConv, Focus, BottleneckCSP, Concat\n",
    "from models.experimental import MixConv2d, CrossConv, C3\n",
    "from utils.general import check_anchor_order, make_divisible, check_file, set_logging\n",
    "from utils.torch_utils import (\n",
    "    time_synchronized, fuse_conv_and_bn, model_info, scale_img, initialize_weights, select_device)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detect(nn.Module):\n",
    "    stride = None  # strides computed during build\n",
    "    export = False  # onnx export\n",
    "\n",
    "    def __init__(self, nc=80, anchors=(), ch=()):  # detection layer\n",
    "        super(Detect, self).__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.no = nc + 5  # number of outputs per anchor\n",
    "        self.nl = len(anchors)  # number of detection layers\n",
    "        self.na = len(anchors[0]) // 2  # number of anchors\n",
    "        self.grid = [torch.zeros(1)] * self.nl  # init grid\n",
    "        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\n",
    "        self.register_buffer('anchors', a)  # shape(nl,na,2)\n",
    "        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\n",
    "        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.copy()  # for profiling\n",
    "        z = []  # inference output\n",
    "        self.training |= self.export\n",
    "        for i in range(self.nl):\n",
    "            x[i] = self.m[i](x[i])  # conv\n",
    "            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
    "            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "            if not self.training:  # inference\n",
    "                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
    "                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\n",
    "\n",
    "                y = x[i].sigmoid()\n",
    "                y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i].to(x[i].device)) * self.stride[i]  # xy\n",
    "                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                z.append(y.view(bs, -1, self.no))\n",
    "\n",
    "        return x if self.training else (torch.cat(z, 1), x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx=20, ny=20):\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None):  # model, input channels, number of classes\n",
    "        super(Model, self).__init__()\n",
    "        if isinstance(cfg, dict):\n",
    "            self.yaml = cfg  # model dict\n",
    "        else:  # is *.yaml\n",
    "            import yaml  # for torch hub\n",
    "            self.yaml_file = Path(cfg).name\n",
    "            with open(cfg) as f:\n",
    "                self.yaml = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "\n",
    "        # Define model\n",
    "        if nc and nc != self.yaml['nc']:\n",
    "            print('Overriding %s nc=%g with nc=%g' % (cfg, self.yaml['nc'], nc))\n",
    "            self.yaml['nc'] = nc  # override yaml value\n",
    "        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist, ch_out\n",
    "        # print([x.shape for x in self.forward(torch.zeros(1, ch, 64, 64))])\n",
    "\n",
    "        # Build strides, anchors\n",
    "        m = self.model[-1]  # Detect()\n",
    "        if isinstance(m, Detect):\n",
    "            s = 128  # 2x min stride\n",
    "            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n",
    "            m.anchors /= m.stride.view(-1, 1, 1)\n",
    "            check_anchor_order(m)\n",
    "            self.stride = m.stride\n",
    "            self._initialize_biases()  # only run once\n",
    "            # print('Strides: %s' % m.stride.tolist())\n",
    "\n",
    "        # Init weights, biases\n",
    "        initialize_weights(self)\n",
    "        self.info()\n",
    "        print('')\n",
    "\n",
    "    def forward(self, x, augment=False, profile=False):\n",
    "        if augment:\n",
    "            img_size = x.shape[-2:]  # height, width\n",
    "            s = [1, 0.83, 0.67]  # scales\n",
    "            f = [None, 3, None]  # flips (2-ud, 3-lr)\n",
    "            y = []  # outputs\n",
    "            for si, fi in zip(s, f):\n",
    "                xi = scale_img(x.flip(fi) if fi else x, si)\n",
    "                yi = self.forward_once(xi)[0]  # forward\n",
    "                # cv2.imwrite('img%g.jpg' % s, 255 * xi[0].numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\n",
    "                yi[..., :4] /= si  # de-scale\n",
    "                if fi == 2:\n",
    "                    yi[..., 1] = img_size[0] - yi[..., 1]  # de-flip ud\n",
    "                elif fi == 3:\n",
    "                    yi[..., 0] = img_size[1] - yi[..., 0]  # de-flip lr\n",
    "                y.append(yi)\n",
    "            return torch.cat(y, 1), None  # augmented inference, train\n",
    "        else:\n",
    "            return self.forward_once(x, profile)  # single-scale inference, train\n",
    "\n",
    "    def forward_once(self, x, profile=False):\n",
    "        y, dt = [], []  # outputs\n",
    "        for m in self.model:\n",
    "            if m.f != -1:  # if not from previous layer\n",
    "                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n",
    "\n",
    "            if profile:\n",
    "                try:\n",
    "                    import thop\n",
    "                    o = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # FLOPS\n",
    "                except:\n",
    "                    o = 0\n",
    "                t = time_synchronized()\n",
    "                for _ in range(10):\n",
    "                    _ = m(x)\n",
    "                dt.append((time_synchronized() - t) * 100)\n",
    "                print('%10.1f%10.0f%10.1fms %-40s' % (o, m.np, dt[-1], m.type))\n",
    "\n",
    "            x = m(x)  # run\n",
    "            y.append(x if m.i in self.save else None)  # save output\n",
    "\n",
    "        if profile:\n",
    "            print('%.1fms total' % sum(dt))\n",
    "        return x\n",
    "\n",
    "    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\n",
    "        m = self.model[-1]  # Detect() module\n",
    "        for mi, s in zip(m.m, m.stride):  # from\n",
    "            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\n",
    "            b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n",
    "            b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n",
    "            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n",
    "\n",
    "    def _print_biases(self):\n",
    "        m = self.model[-1]  # Detect() module\n",
    "        for mi in m.m:  # from\n",
    "            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)\n",
    "            print(('%6g Conv2d.bias:' + '%10.3g' * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))\n",
    "\n",
    "    # def _print_weights(self):\n",
    "    #     for m in self.model.modules():\n",
    "    #         if type(m) is Bottleneck:\n",
    "    #             print('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights\n",
    "\n",
    "    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\n",
    "        print('Fusing layers... ')\n",
    "        for m in self.model.modules():\n",
    "            if type(m) is Conv:\n",
    "                m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatability\n",
    "                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n",
    "                delattr(m, 'bn')  # remove batchnorm\n",
    "                m.forward = m.fuseforward  # update forward\n",
    "        self.info()\n",
    "        return self\n",
    "\n",
    "    def info(self, verbose=False):  # print model information\n",
    "        model_info(self, verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model(d, ch):  # model_dict, input_channels(3)\n",
    "    logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n",
    "    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n",
    "    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n",
    "    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n",
    "\n",
    "    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
    "    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n",
    "        m = eval(m) if isinstance(m, str) else m  # eval strings\n",
    "        for j, a in enumerate(args):\n",
    "            try:\n",
    "                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n",
    "        if m in [nn.Conv2d, Conv, Bottleneck, SPP, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3]:\n",
    "            c1, c2 = ch[f], args[0]\n",
    "\n",
    "            # Normal\n",
    "            # if i > 0 and args[0] != no:  # channel expansion factor\n",
    "            #     ex = 1.75  # exponential (default 2.0)\n",
    "            #     e = math.log(c2 / ch[1]) / math.log(2)\n",
    "            #     c2 = int(ch[1] * ex ** e)\n",
    "            # if m != Focus:\n",
    "\n",
    "            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2\n",
    "\n",
    "            # Experimental\n",
    "            # if i > 0 and args[0] != no:  # channel expansion factor\n",
    "            #     ex = 1 + gw  # exponential (default 2.0)\n",
    "            #     ch1 = 32  # ch[1]\n",
    "            #     e = math.log(c2 / ch1) / math.log(2)  # level 1-n\n",
    "            #     c2 = int(ch1 * ex ** e)\n",
    "            # if m != Focus:\n",
    "            #     c2 = make_divisible(c2, 8) if c2 != no else c2\n",
    "\n",
    "            args = [c1, c2, *args[1:]]\n",
    "            if m in [BottleneckCSP, C3]:\n",
    "                args.insert(2, n)\n",
    "                n = 1\n",
    "        elif m is nn.BatchNorm2d:\n",
    "            args = [ch[f]]\n",
    "        elif m is Concat:\n",
    "            c2 = sum([ch[-1 if x == -1 else x + 1] for x in f])\n",
    "        elif m is Detect:\n",
    "            args.append([ch[x + 1] for x in f])\n",
    "            if isinstance(args[1], int):  # number of anchors\n",
    "                args[1] = [list(range(args[1] * 2))] * len(f)\n",
    "        else:\n",
    "            c2 = ch[f]\n",
    "\n",
    "        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\n",
    "        t = str(m)[8:-2].replace('__main__.', '')  # module type\n",
    "        np = sum([x.numel() for x in m_.parameters()])  # number params\n",
    "        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n",
    "        logger.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\n",
    "        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n",
    "        layers.append(m_)\n",
    "        ch.append(c2)\n",
    "    return nn.Sequential(*layers), sorted(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--cfg', type=str, default='yolov5s.yaml', help='model.yaml')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    opt = parser.parse_args()\n",
    "    opt.cfg = check_file(opt.cfg)  # check file\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(opt.cfg).to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Profile\n",
    "    # img = torch.rand(8 if torch.cuda.is_available() else 1, 3, 640, 640).to(device)\n",
    "    # y = model(img, profile=True)\n",
    "\n",
    "    # ONNX export\n",
    "    # model.model[-1].export = True\n",
    "    # torch.onnx.export(model, img, opt.cfg.replace('.yaml', '.onnx'), verbose=True, opset_version=11)\n",
    "\n",
    "    # Tensorboard\n",
    "    # from torch.utils.tensorboard import SummaryWriter\n",
    "    # tb_writer = SummaryWriter()\n",
    "    # print(\"Run 'tensorboard --logdir=models/runs' to view tensorboard at http://localhost:6006/\")\n",
    "    # tb_writer.add_graph(model.model, img)  # add model to tensorboard\n",
    "    # tb_writer.add_image('test', img[0], dataformats='CWH')  # add model to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
